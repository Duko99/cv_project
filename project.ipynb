{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad62cb3-b86d-4a0a-8492-ef09608a4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix last dense layer (should be 10 but only accespts 8). Is it the number of classes? If so then nothing to fix\n",
    "# TODO: discuss how reading in dataset could be made multi-threaded (each dataset is read in on own thread then all joined together at the end)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import imageio as iio\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from numba import cuda  # https://stackoverflow.com/a/52354865/6476994\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4a177c-422b-48b4-b51f-ac05542a4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows all images to be displayed at once (else only displays the last call to plt.imshow())\n",
    "# https://stackoverflow.com/a/41210974\n",
    "def displayImage(image, caption = None, colour = None) -> None:\n",
    "    plt.figure()\n",
    "    if(colour != None):\n",
    "        plt.imshow(image, cmap=colour)\n",
    "    else:\n",
    "        plt.imshow(image)\n",
    "        \n",
    "    if(caption != None):\n",
    "        # display caption below picture (https://stackoverflow.com/a/51486361)\n",
    "        plt.figtext(0.5, 0.01, caption, wrap=True, horizontalalignment='center', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7119a4c7-53e6-47b3-a57c-c8c54758002a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_names: ['BB01', 'BB02', 'BB03', 'BB04', 'BB05', 'BB06', 'BB07', 'BB08', 'BB09', 'BB10', 'BB11', 'BB12', 'BB13', 'BB14', 'BB15', 'BB16', 'BB17', 'BB18', 'BB19', 'BB20']\n"
     ]
    }
   ],
   "source": [
    "dataset_names = [\"BB01\", \"BB02\", \"BB03\", \"BB04\", \"BB05\", \"BB06\", \"BB07\", \"BB08\", \"BB09\", \"BB10\"]\n",
    "# dataset_names = [\"BB08\"]\n",
    "# dataset_names = []\n",
    "for i in range(11, 21):\n",
    "    dataset_names.append(\"BB{}\".format(i))\n",
    "print(\"dataset_names: {}\".format(dataset_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2162b806-d6bd-47ba-acc3-3f7209a27997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up GPU if it didn't after the last run\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b197fb12-a89b-4f03-a28c-ca0cb72b9e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = tf.keras.datasets.mnist\n",
    "# (training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760dbfe7-ea61-44ed-bbbd-1d3d3b576c3f",
   "metadata": {},
   "source": [
    "# Read in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfbb46df-237c-4f68-b1af-5ee75f6b69b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in images for dataset: BB01\n",
      "all_image_filenames length: 285\n",
      "done current dataset\n",
      "reading in images for dataset: BB02\n",
      "all_image_filenames length: 45\n",
      "done current dataset\n",
      "reading in images for dataset: BB03\n",
      "all_image_filenames length: 230\n",
      "done current dataset\n",
      "reading in images for dataset: BB04\n",
      "all_image_filenames length: 999\n",
      "done current dataset\n",
      "reading in images for dataset: BB05\n",
      "all_image_filenames length: 189\n",
      "done current dataset\n",
      "reading in images for dataset: BB06\n",
      "all_image_filenames length: 1137\n",
      "done current dataset\n",
      "reading in images for dataset: BB07\n",
      "all_image_filenames length: 324\n",
      "done current dataset\n",
      "reading in images for dataset: BB08\n",
      "all_image_filenames length: 66\n",
      "done current dataset\n",
      "reading in images for dataset: BB09\n",
      "all_image_filenames length: 357\n",
      "done current dataset\n",
      "reading in images for dataset: BB10\n",
      "all_image_filenames length: 121\n",
      "done current dataset\n",
      "reading in images for dataset: BB11\n",
      "all_image_filenames length: 167\n",
      "done current dataset\n",
      "reading in images for dataset: BB12\n",
      "all_image_filenames length: 157\n",
      "done current dataset\n",
      "reading in images for dataset: BB13\n",
      "all_image_filenames length: 50\n",
      "done current dataset\n",
      "reading in images for dataset: BB14\n",
      "all_image_filenames length: 367\n",
      "done current dataset\n",
      "reading in images for dataset: BB15\n",
      "all_image_filenames length: 91\n",
      "done current dataset\n",
      "reading in images for dataset: BB16\n",
      "all_image_filenames length: 213\n",
      "done current dataset\n",
      "reading in images for dataset: BB17\n",
      "all_image_filenames length: 26\n",
      "done current dataset\n",
      "reading in images for dataset: BB18\n",
      "all_image_filenames length: 75\n",
      "done current dataset\n",
      "reading in images for dataset: BB19\n",
      "all_image_filenames length: 72\n",
      "done current dataset\n",
      "reading in images for dataset: BB20\n",
      "all_image_filenames length: 96\n",
      "done current dataset\n"
     ]
    }
   ],
   "source": [
    "# get the all original output filenames\n",
    "def readInImages(datasetName):\n",
    "    print(\"reading in images for dataset: {}\".format(datasetName))\n",
    "    desired_size = 224\n",
    "    image_list = []\n",
    "    imgRegExp = re.compile(r'.*[.](JPG)$')\n",
    "    # https://stackoverflow.com/a/3207973\n",
    "    all_image_filenames = next(os.walk('data/{}'.format(datasetName)),\n",
    "                         (None, None, []))[2]  # [] if no file\n",
    "    # filter out file names that are not JPEGs\n",
    "    all_image_filenames = [i for i in all_image_filenames if imgRegExp.match(i)]\n",
    "    # walk() outputs unordered, so we need to sort\n",
    "    all_image_filenames.sort()\n",
    "    # print(\"all_image_filenames: {}\".format(all_image_filenames))\n",
    "    print(\"all_image_filenames length: {}\".format(len(all_image_filenames)))\n",
    "    for fn in all_image_filenames:\n",
    "        # im = Image.open('data/{}/{}'.format(datasetName, fn))\n",
    "        im = cv2.imread('data/{}/{}'.format(datasetName, fn))\n",
    "        # resize the image to conserve memory, and transform it to be square while\n",
    "        # maintaining the aspect ration (give it padding):\n",
    "        # https://jdhao.github.io/2017/11/06/resize-image-to-square-with-padding/#using-opencv\n",
    "        # im = cv2.resize(im, (480, 270), interpolation=cv2.INTER_CUBIC)\n",
    "        # im = cv2.resize(im, (240, 135), interpolation=cv2.INTER_CUBIC)\n",
    "        # im = cv2.resize(im, (192, 108), interpolation=cv2.INTER_CUBIC)\n",
    "        old_size = im.shape[:2]\n",
    "        ratio = float(desired_size)/max(old_size)\n",
    "        new_size = tuple([int(x*ratio) for x in old_size])\n",
    "        im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "\n",
    "        delta_w = desired_size - new_size[1]\n",
    "        delta_h = desired_size - new_size[0]\n",
    "        top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "        left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "\n",
    "        color = [0, 0, 0]\n",
    "        new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "\n",
    "        \n",
    "        image_list.append(np.asarray(new_im))\n",
    "    \n",
    "    print(\"done current dataset\")\n",
    "    return image_list\n",
    "\n",
    "all_images = []\n",
    "for fn in dataset_names:    \n",
    "    all_images = [*all_images, *readInImages(fn)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded9195-def1-48e6-bb54-4062ec01dc5b",
   "metadata": {},
   "source": [
    "# Read in dataset's labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d14a993b-efcc-4927-b522-44cbb90e8295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all classes (length=8): {'Human Presense/Deployment', 'Other', 'Cat', 'Fox', 'Emu', 'Kangaroo', 'Rabbit', 'Empty photo'}\n"
     ]
    }
   ],
   "source": [
    "# labels (using dataset's CSV file)\n",
    "\n",
    "def readInAnnotations(datasetName):\n",
    "    labelList = []\n",
    "    # https://realpython.com/python-csv/#reading-csv-files-with-csv\n",
    "    with open('data/{}/{}.csv'.format(datasetName, datasetName)) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            # print(\"row: {}\".format(row))\n",
    "            # first row always contains this string, so ignore it\n",
    "            if \"RECONYX - MapView Professional\" in row:\n",
    "                continue\n",
    "            if line_count == 0:\n",
    "                # print(f'Column names are {\", \".join(row)}')\n",
    "                line_count += 1\n",
    "            else:\n",
    "                # print(\"Image Name: {}. Hit List: {}\".format(row[0], row[22].replace(\"\\n\", \", \")))\n",
    "                # FIXME handle when hitlist contains more than one item (e.g., BB06 IMG_512 has 'kangaroo' and 'empty photo') - sort of handled, need to make more dynamic\n",
    "                hit_list = row[22]\n",
    "                if hit_list == '':\n",
    "                    labelList.append(\"Empty photo\")\n",
    "                elif hit_list == 'Empty photo\\nHuman Presense/Deployment':\n",
    "                    labelList.append(\"Human Presense/Deployment\")\n",
    "                elif hit_list == 'Kangaroo\\nEmpty photo':\n",
    "                    labelList.append(\"Kangaroo\")\n",
    "                else:\n",
    "                    # FIXME: rendundant case?\n",
    "                    labelList.append(hit_list.replace(\"\\n\", \", \"))\n",
    "                line_count += 1\n",
    "    # print(\"returning labelList (length: {}): {}\".format(len(labelList), labelList))\n",
    "    # print(\"returning labelList of length: {}\".format(len(labelList)))\n",
    "    return labelList\n",
    "\n",
    "all_image_labels = []\n",
    "for fn in dataset_names:\n",
    "    all_image_labels = [*all_image_labels, *readInAnnotations(fn)]\n",
    "\n",
    "# print(\"all_image_labels: {}\".format(all_image_labels))\n",
    "\n",
    "classes = set(all_image_labels)\n",
    "print(\"all classes (length={}): {}\".format(len(classes), classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f21c8c-b002-41c6-8ee5-8c249bc7f2aa",
   "metadata": {},
   "source": [
    "# Randomly split the dataset and corresponding labels into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44e9f9bb-0adf-4b24-8bb7-6607dccd3ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_images size: 5067\n",
      "training_labels length: 4053\n",
      "test_labels length: 1014\n",
      "training_classes (length=8): {'Human Presense/Deployment', 'Other', 'Cat', 'Fox', 'Emu', 'Kangaroo', 'Rabbit', 'Empty photo'}\n",
      "test_classes (length=8): {'Human Presense/Deployment', 'Other', 'Cat', 'Fox', 'Emu', 'Kangaroo', 'Rabbit', 'Empty photo'}\n",
      "done stacking\n",
      "training_images shape: (4053, 224, 224, 3)\n",
      "test_images shape: (1014, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"all_images size: {}\".format(len(all_images)))\n",
    "# print(\"all_image_labels size: {}\".format(len(all_image_labels)))\n",
    "\n",
    "\n",
    "training_images, test_images, training_labels, test_labels = train_test_split(all_images, all_image_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"training_labels length: {}\".format(len(training_labels)))\n",
    "print(\"test_labels length: {}\".format(len(test_labels)))\n",
    "\n",
    "training_classes = set(training_labels)\n",
    "test_classes = set(test_labels)\n",
    "print(\"training_classes (length={}): {}\".format(len(training_classes), training_classes))\n",
    "print(\"test_classes (length={}): {}\".format(len(test_classes), test_classes))\n",
    "\n",
    "# integer-encode labels so they can be one-hot-encoded\n",
    "# https://stackoverflow.com/a/56227965/6476994\n",
    "label_encoder = LabelEncoder()\n",
    "training_labels = np.array(training_labels)\n",
    "training_labels = label_encoder.fit_transform(training_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "test_labels = label_encoder.fit_transform(test_labels)\n",
    "\n",
    "\n",
    "# convert list of numpy arrays to numpy array of numpy arrays\n",
    "# https://stackoverflow.com/a/27516930/6476994\n",
    "training_images = np.stack(training_images, axis = 0)\n",
    "test_images = np.stack(test_images, axis = 0)\n",
    "\n",
    "print(\"done stacking\")\n",
    "print(\"training_images shape: {}\".format(training_images.shape))\n",
    "print(\"test_images shape: {}\".format(test_images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de40603-e6cd-4c93-856c-eed237c3f7dc",
   "metadata": {},
   "source": [
    "# ZFNet\n",
    "\n",
    "Source: https://towardsdatascience.com/zfnet-an-explanation-of-paper-with-code-f1bd6752121d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da469ee2-ad90-402b-9b57-55a1388138e3",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d882d77-9109-494a-8c37-7ee3518adf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-28 09:34:08.470131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:34:08.473566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:34:08.473801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:34:08.474619: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-28 09:34:08.474976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:34:08.475137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:34:08.475272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:34:08.792788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:34:08.792964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:34:08.793112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:34:08.793232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5748 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:2b:00.0, compute capability: 7.5\n",
      "2022-05-28 09:34:09.184853: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2440359936 exceeds 10% of free system memory.\n",
      "/home/luke/miniconda3/envs/cv-project-env/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "2022-05-28 09:34:11.201125: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1952047104 exceeds 10% of free system memory.\n",
      "2022-05-28 09:34:11.832428: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1952047104 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-28 09:34:13.236682: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2022-05-28 09:34:13.563447: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 9s 214ms/step - loss: 1.5686 - accuracy: 0.5608 - top_k_categorical_accuracy: 0.9565 - val_loss: 1.3435 - val_accuracy: 0.6412 - val_top_k_categorical_accuracy: 0.9840 - lr: 0.0100\n",
      "Epoch 2/90\n",
      "26/26 [==============================] - 4s 162ms/step - loss: 1.0667 - accuracy: 0.6410 - top_k_categorical_accuracy: 0.9861 - val_loss: 1.0795 - val_accuracy: 0.6449 - val_top_k_categorical_accuracy: 0.9901 - lr: 0.0100\n",
      "Epoch 3/90\n",
      "26/26 [==============================] - 4s 162ms/step - loss: 0.9892 - accuracy: 0.6481 - top_k_categorical_accuracy: 0.9907 - val_loss: 0.9682 - val_accuracy: 0.6671 - val_top_k_categorical_accuracy: 0.9889 - lr: 0.0100\n",
      "Epoch 4/90\n",
      "26/26 [==============================] - 4s 163ms/step - loss: 0.9136 - accuracy: 0.6672 - top_k_categorical_accuracy: 0.9920 - val_loss: 0.9017 - val_accuracy: 0.6930 - val_top_k_categorical_accuracy: 0.9889 - lr: 0.0100\n",
      "Epoch 5/90\n",
      "26/26 [==============================] - 4s 163ms/step - loss: 0.8636 - accuracy: 0.6808 - top_k_categorical_accuracy: 0.9920 - val_loss: 0.9005 - val_accuracy: 0.7053 - val_top_k_categorical_accuracy: 0.9889 - lr: 0.0100\n",
      "Epoch 6/90\n",
      "26/26 [==============================] - 4s 163ms/step - loss: 0.7618 - accuracy: 0.7255 - top_k_categorical_accuracy: 0.9948 - val_loss: 0.7513 - val_accuracy: 0.7435 - val_top_k_categorical_accuracy: 0.9889 - lr: 0.0100\n",
      "Epoch 7/90\n",
      "26/26 [==============================] - 4s 162ms/step - loss: 0.7288 - accuracy: 0.7381 - top_k_categorical_accuracy: 0.9938 - val_loss: 0.7968 - val_accuracy: 0.7189 - val_top_k_categorical_accuracy: 0.9914 - lr: 0.0100\n",
      "Epoch 8/90\n",
      "26/26 [==============================] - 4s 164ms/step - loss: 0.7303 - accuracy: 0.7338 - top_k_categorical_accuracy: 0.9948 - val_loss: 0.7209 - val_accuracy: 0.7645 - val_top_k_categorical_accuracy: 0.9877 - lr: 1.0000e-03\n",
      "Epoch 9/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.6171 - accuracy: 0.7804 - top_k_categorical_accuracy: 0.9966 - val_loss: 0.6696 - val_accuracy: 0.7842 - val_top_k_categorical_accuracy: 0.9914 - lr: 1.0000e-03\n",
      "Epoch 10/90\n",
      "26/26 [==============================] - 4s 165ms/step - loss: 0.5723 - accuracy: 0.8010 - top_k_categorical_accuracy: 0.9963 - val_loss: 0.6440 - val_accuracy: 0.7916 - val_top_k_categorical_accuracy: 0.9901 - lr: 1.0000e-03\n",
      "Epoch 11/90\n",
      "26/26 [==============================] - 4s 165ms/step - loss: 0.5520 - accuracy: 0.8106 - top_k_categorical_accuracy: 0.9960 - val_loss: 0.6336 - val_accuracy: 0.7916 - val_top_k_categorical_accuracy: 0.9914 - lr: 1.0000e-03\n",
      "Epoch 12/90\n",
      "26/26 [==============================] - 4s 165ms/step - loss: 0.5337 - accuracy: 0.8211 - top_k_categorical_accuracy: 0.9969 - val_loss: 0.6166 - val_accuracy: 0.8015 - val_top_k_categorical_accuracy: 0.9901 - lr: 1.0000e-03\n",
      "Epoch 13/90\n",
      "26/26 [==============================] - 4s 165ms/step - loss: 0.5213 - accuracy: 0.8263 - top_k_categorical_accuracy: 0.9969 - val_loss: 0.6059 - val_accuracy: 0.8064 - val_top_k_categorical_accuracy: 0.9914 - lr: 1.0000e-03\n",
      "Epoch 14/90\n",
      "26/26 [==============================] - 4s 165ms/step - loss: 0.5132 - accuracy: 0.8251 - top_k_categorical_accuracy: 0.9960 - val_loss: 0.5872 - val_accuracy: 0.8039 - val_top_k_categorical_accuracy: 0.9914 - lr: 1.0000e-03\n",
      "Epoch 15/90\n",
      "26/26 [==============================] - 4s 165ms/step - loss: 0.4879 - accuracy: 0.8319 - top_k_categorical_accuracy: 0.9969 - val_loss: 0.5856 - val_accuracy: 0.8027 - val_top_k_categorical_accuracy: 0.9926 - lr: 1.0000e-03\n",
      "Epoch 16/90\n",
      "26/26 [==============================] - 4s 165ms/step - loss: 0.4846 - accuracy: 0.8300 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5628 - val_accuracy: 0.8015 - val_top_k_categorical_accuracy: 0.9914 - lr: 1.0000e-03\n",
      "Epoch 17/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4768 - accuracy: 0.8418 - top_k_categorical_accuracy: 0.9966 - val_loss: 0.5595 - val_accuracy: 0.8089 - val_top_k_categorical_accuracy: 0.9926 - lr: 1.0000e-03\n",
      "Epoch 18/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4614 - accuracy: 0.8387 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5661 - val_accuracy: 0.8138 - val_top_k_categorical_accuracy: 0.9926 - lr: 1.0000e-03\n",
      "Epoch 19/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4485 - accuracy: 0.8433 - top_k_categorical_accuracy: 0.9966 - val_loss: 0.5505 - val_accuracy: 0.8089 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-04\n",
      "Epoch 20/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4386 - accuracy: 0.8479 - top_k_categorical_accuracy: 0.9969 - val_loss: 0.5462 - val_accuracy: 0.8126 - val_top_k_categorical_accuracy: 0.9926 - lr: 1.0000e-04\n",
      "Epoch 21/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4311 - accuracy: 0.8516 - top_k_categorical_accuracy: 0.9969 - val_loss: 0.5406 - val_accuracy: 0.8089 - val_top_k_categorical_accuracy: 0.9926 - lr: 1.0000e-04\n",
      "Epoch 22/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4292 - accuracy: 0.8523 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5411 - val_accuracy: 0.8089 - val_top_k_categorical_accuracy: 0.9926 - lr: 1.0000e-04\n",
      "Epoch 23/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4264 - accuracy: 0.8523 - top_k_categorical_accuracy: 0.9975 - val_loss: 0.5432 - val_accuracy: 0.8089 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 24/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4265 - accuracy: 0.8516 - top_k_categorical_accuracy: 0.9975 - val_loss: 0.5423 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 25/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4258 - accuracy: 0.8523 - top_k_categorical_accuracy: 0.9975 - val_loss: 0.5415 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 26/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4253 - accuracy: 0.8523 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5409 - val_accuracy: 0.8089 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 27/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4249 - accuracy: 0.8526 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5403 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 28/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4248 - accuracy: 0.8523 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5399 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 29/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4247 - accuracy: 0.8529 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5397 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 30/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4244 - accuracy: 0.8535 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5398 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 31/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4243 - accuracy: 0.8532 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5397 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 32/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4241 - accuracy: 0.8535 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5394 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 33/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4240 - accuracy: 0.8529 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5392 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 34/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4238 - accuracy: 0.8532 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5390 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 35/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4236 - accuracy: 0.8535 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5388 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 36/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4236 - accuracy: 0.8538 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5390 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 37/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4233 - accuracy: 0.8535 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5386 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 38/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4232 - accuracy: 0.8532 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5387 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 39/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4231 - accuracy: 0.8532 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5385 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 40/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4231 - accuracy: 0.8535 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5383 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 41/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4228 - accuracy: 0.8535 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5383 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 42/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4227 - accuracy: 0.8538 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5384 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 43/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4226 - accuracy: 0.8544 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5384 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 44/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4224 - accuracy: 0.8541 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5382 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 45/90\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.4222 - accuracy: 0.8547 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5377 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 46/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4221 - accuracy: 0.8550 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5376 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 47/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4219 - accuracy: 0.8547 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5377 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 48/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4218 - accuracy: 0.8541 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5377 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 49/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4217 - accuracy: 0.8544 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5374 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 50/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4216 - accuracy: 0.8541 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5373 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 51/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4215 - accuracy: 0.8544 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5374 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 52/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4213 - accuracy: 0.8547 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5374 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 53/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4212 - accuracy: 0.8547 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5373 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 54/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4210 - accuracy: 0.8553 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5370 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 55/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4209 - accuracy: 0.8556 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5366 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 56/90\n",
      "26/26 [==============================] - 4s 168ms/step - loss: 0.4208 - accuracy: 0.8556 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5366 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 57/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4207 - accuracy: 0.8550 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5368 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 58/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4205 - accuracy: 0.8556 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5367 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 59/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4204 - accuracy: 0.8553 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5365 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 60/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4203 - accuracy: 0.8560 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5362 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 61/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4201 - accuracy: 0.8560 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5360 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 62/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4200 - accuracy: 0.8556 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5362 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 63/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4199 - accuracy: 0.8560 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5361 - val_accuracy: 0.8101 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 64/90\n",
      "26/26 [==============================] - 4s 168ms/step - loss: 0.4197 - accuracy: 0.8560 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5359 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 65/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4197 - accuracy: 0.8560 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5356 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 66/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4195 - accuracy: 0.8560 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5359 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 67/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4193 - accuracy: 0.8560 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5357 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 68/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4193 - accuracy: 0.8563 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5354 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 69/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4191 - accuracy: 0.8563 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5356 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 70/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4190 - accuracy: 0.8563 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5352 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 71/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4188 - accuracy: 0.8560 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5354 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 72/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4187 - accuracy: 0.8556 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5354 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 73/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4186 - accuracy: 0.8556 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5350 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 74/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4185 - accuracy: 0.8556 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5350 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 75/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4183 - accuracy: 0.8563 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5348 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 76/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4182 - accuracy: 0.8566 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5346 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 77/90\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.4181 - accuracy: 0.8563 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5347 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 78/90\n",
      "26/26 [==============================] - 4s 168ms/step - loss: 0.4180 - accuracy: 0.8563 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5345 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 79/90\n",
      "26/26 [==============================] - 4s 172ms/step - loss: 0.4179 - accuracy: 0.8566 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5347 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 80/90\n",
      "26/26 [==============================] - 4s 170ms/step - loss: 0.4177 - accuracy: 0.8566 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5344 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 81/90\n",
      "26/26 [==============================] - 4s 162ms/step - loss: 0.4176 - accuracy: 0.8572 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5341 - val_accuracy: 0.8126 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 82/90\n",
      "26/26 [==============================] - 4s 159ms/step - loss: 0.4174 - accuracy: 0.8566 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5342 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 83/90\n",
      "26/26 [==============================] - 4s 161ms/step - loss: 0.4173 - accuracy: 0.8566 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5341 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 84/90\n",
      "26/26 [==============================] - 4s 159ms/step - loss: 0.4172 - accuracy: 0.8569 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5338 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 85/90\n",
      "26/26 [==============================] - 4s 160ms/step - loss: 0.4171 - accuracy: 0.8572 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5339 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 86/90\n",
      "26/26 [==============================] - 4s 160ms/step - loss: 0.4171 - accuracy: 0.8560 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5334 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 87/90\n",
      "26/26 [==============================] - 4s 159ms/step - loss: 0.4168 - accuracy: 0.8563 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5339 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 88/90\n",
      "26/26 [==============================] - 4s 160ms/step - loss: 0.4167 - accuracy: 0.8572 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5337 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 89/90\n",
      "26/26 [==============================] - 4s 159ms/step - loss: 0.4165 - accuracy: 0.8566 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5335 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n",
      "Epoch 90/90\n",
      "26/26 [==============================] - 4s 159ms/step - loss: 0.4165 - accuracy: 0.8566 - top_k_categorical_accuracy: 0.9972 - val_loss: 0.5333 - val_accuracy: 0.8113 - val_top_k_categorical_accuracy: 0.9938 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f248845e3a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_images = tf.map_fn(lambda i: tf.stack([i]*3, axis=-1), training_images).numpy()\n",
    "# test_images = tf.map_fn(lambda i: tf.stack([i]*3, axis=-1), test_images).numpy()\n",
    "\n",
    "training_images = tf.image.resize(training_images, [224, 224]).numpy()\n",
    "test_images = tf.image.resize(test_images, [224, 224]).numpy()\n",
    "\n",
    "training_images = training_images.reshape(training_images.shape)\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images.reshape(test_images.shape)\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "training_labels = tf.keras.utils.to_categorical(training_labels, num_classes=len(training_classes))\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=len(test_classes))\n",
    "\n",
    "num_len_train = int(0.8 * len(training_images))\n",
    "\n",
    "ttraining_images = training_images[:num_len_train]\n",
    "ttraining_labels = training_labels[:num_len_train]\n",
    "\n",
    "valid_images = training_images[num_len_train:]\n",
    "valid_labels = training_labels[num_len_train:]\n",
    "\n",
    "training_images = ttraining_images\n",
    "training_labels = ttraining_labels\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "                                    \n",
    "\t\ttf.keras.layers.Conv2D(96, (7, 7), strides=(2, 2), activation='relu',\n",
    "\t\t\tinput_shape=(224, 224, 3)),\n",
    "\t\ttf.keras.layers.MaxPooling2D(3, strides=2),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.image.per_image_standardization(x)),\n",
    "\n",
    "\t\ttf.keras.layers.Conv2D(256, (5, 5), strides=(2, 2), activation='relu'),\n",
    "\t\ttf.keras.layers.MaxPooling2D(3, strides=2),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.image.per_image_standardization(x)),\n",
    "\n",
    "\t\ttf.keras.layers.Conv2D(384, (3, 3), activation='relu'),\n",
    "\n",
    "\t\ttf.keras.layers.Conv2D(384, (3, 3), activation='relu'),\n",
    "\n",
    "\t\ttf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "\n",
    "\t\ttf.keras.layers.MaxPooling2D(3, strides=2),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "\t\ttf.keras.layers.Dense(4096),\n",
    "\n",
    "\t\ttf.keras.layers.Dense(4096),\n",
    "\n",
    "\t\ttf.keras.layers.Dense(len(classes), activation='softmax')#FIXME is this the number of classes? (check paper)\n",
    "\t])\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.01, momentum=0.9), \\\n",
    "              loss='categorical_crossentropy', \\\n",
    "              metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(5)])\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \\\n",
    "                                            \t\tfactor=0.1, patience=1, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmin_lr=0.00001)\n",
    "\n",
    "model.fit(training_images, training_labels, batch_size=128, \\\n",
    "          validation_data=(valid_images, valid_labels), \\\n",
    "\t\t\t\t\tepochs=90, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d43d1c-41ee-45f0-a341-3d2757eb5a12",
   "metadata": {},
   "source": [
    "## Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7c21960-4a3d-46cb-8dfd-d3b8a8314ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_images shape: (1014, 224, 224, 3)\n",
      "test_labels shape: (1014, 8)\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.5641 - accuracy: 0.8245 - top_k_categorical_accuracy: 0.9931\n"
     ]
    }
   ],
   "source": [
    "print('test_images shape: {}'.format(test_images.shape))\n",
    "print('test_labels shape: {}'.format(test_labels.shape))\n",
    "\n",
    "results = model.evaluate(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18b52e25-9d06-4e81-b6c7-d317e158b35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 14ms/step\n",
      "Predictions (shape: (1014, 8)):\n",
      "[[0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "predictions = (model.predict(test_images) > 0.5).astype(\"int32\")\n",
    "print(\"Predictions (shape: {}):\\n{}\".format(predictions.shape, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37a2dd7f-3232-4b86-84e0-4e4b7137660b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         9\n",
      "           1       0.77      0.54      0.64       160\n",
      "           2       0.87      0.71      0.78        97\n",
      "           3       0.62      0.29      0.39        28\n",
      "           4       0.79      0.39      0.52        59\n",
      "           5       0.89      0.94      0.92       644\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.87      0.78      0.82      1014\n",
      "   macro avg       0.49      0.36      0.41      1014\n",
      "weighted avg       0.83      0.78      0.80      1014\n",
      " samples avg       0.78      0.78      0.78      1014\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/miniconda3/envs/cv-project-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/luke/miniconda3/envs/cv-project-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, predictions))\n",
    "# TODO resolve classes to their names (current integers in left column)\n",
    "\n",
    "# from sklearn.metrics import precision_score\n",
    "# print(\"Precision score: {}\".format(precision_score(test_labels,predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14e4681e-b9b8-419c-9cd5-bfd3b0a1763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ZFNet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c0524-22ef-4ad4-8403-faec5dc9f5f4",
   "metadata": {},
   "source": [
    "## Free up the GPU's memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b4c59ac-35fd-420c-ab71-2dc2c7c457ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25b0b019-ff12-4c7f-9278-3c16774e1262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# Counter(training_labels.tolist())\n",
    "# Counter(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426f494-49a1-4bac-a74f-35cb460b0175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
