{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad62cb3-b86d-4a0a-8492-ef09608a4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix last dense layer (should be 10 but only accespts 8). Is it the number of classes? If so then nothing to fix\n",
    "# TODO: discuss how reading in dataset could be made multi-threaded (each dataset is read in on own thread then all joined together at the end)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import imageio as iio\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from numba import cuda  # https://stackoverflow.com/a/52354865/6476994\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4a177c-422b-48b4-b51f-ac05542a4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows all images to be displayed at once (else only displays the last call to plt.imshow())\n",
    "# https://stackoverflow.com/a/41210974\n",
    "def displayImage(image, caption = None, colour = None) -> None:\n",
    "    plt.figure()\n",
    "    if(colour != None):\n",
    "        plt.imshow(image, cmap=colour)\n",
    "    else:\n",
    "        plt.imshow(image)\n",
    "        \n",
    "    if(caption != None):\n",
    "        # display caption below picture (https://stackoverflow.com/a/51486361)\n",
    "        plt.figtext(0.5, 0.01, caption, wrap=True, horizontalalignment='center', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7119a4c7-53e6-47b3-a57c-c8c54758002a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_names: ['BB01', 'BB02', 'BB03', 'BB04', 'BB05', 'BB06', 'BB07', 'BB08', 'BB09', 'BB10', 'BB11', 'BB12', 'BB13', 'BB14', 'BB15', 'BB16', 'BB17', 'BB18', 'BB19', 'BB20', 'BB21', 'BB22', 'BB23', 'BB24', 'BB25', 'BB26', 'BB27', 'BB28', 'BB29', 'BB30']\n"
     ]
    }
   ],
   "source": [
    "dataset_names = [\"BB01\", \"BB02\", \"BB03\", \"BB04\", \"BB05\", \"BB06\", \"BB07\", \"BB08\", \"BB09\", \"BB10\"]\n",
    "# dataset_names = [\"BB08\"]\n",
    "# dataset_names = []\n",
    "for i in range(11, 31):\n",
    "    dataset_names.append(\"BB{}\".format(i))\n",
    "print(\"dataset_names: {}\".format(dataset_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2162b806-d6bd-47ba-acc3-3f7209a27997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up GPU if it didn't after the last run\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760dbfe7-ea61-44ed-bbbd-1d3d3b576c3f",
   "metadata": {},
   "source": [
    "# Read in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfbb46df-237c-4f68-b1af-5ee75f6b69b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in images for dataset: BB01\n",
      "all_image_filenames length: 285\n",
      "done current dataset\n",
      "reading in images for dataset: BB02\n",
      "all_image_filenames length: 45\n",
      "done current dataset\n",
      "reading in images for dataset: BB03\n",
      "all_image_filenames length: 230\n",
      "done current dataset\n",
      "reading in images for dataset: BB04\n",
      "all_image_filenames length: 999\n",
      "done current dataset\n",
      "reading in images for dataset: BB05\n",
      "all_image_filenames length: 189\n",
      "done current dataset\n",
      "reading in images for dataset: BB06\n",
      "all_image_filenames length: 1137\n",
      "done current dataset\n",
      "reading in images for dataset: BB07\n",
      "all_image_filenames length: 324\n",
      "done current dataset\n",
      "reading in images for dataset: BB08\n",
      "all_image_filenames length: 66\n",
      "done current dataset\n",
      "reading in images for dataset: BB09\n",
      "all_image_filenames length: 357\n",
      "done current dataset\n",
      "reading in images for dataset: BB10\n",
      "all_image_filenames length: 121\n",
      "done current dataset\n",
      "reading in images for dataset: BB11\n",
      "all_image_filenames length: 167\n",
      "done current dataset\n",
      "reading in images for dataset: BB12\n",
      "all_image_filenames length: 157\n",
      "done current dataset\n",
      "reading in images for dataset: BB13\n",
      "all_image_filenames length: 50\n",
      "done current dataset\n",
      "reading in images for dataset: BB14\n",
      "all_image_filenames length: 367\n",
      "done current dataset\n",
      "reading in images for dataset: BB15\n",
      "all_image_filenames length: 91\n",
      "done current dataset\n",
      "reading in images for dataset: BB16\n",
      "all_image_filenames length: 213\n",
      "done current dataset\n",
      "reading in images for dataset: BB17\n",
      "all_image_filenames length: 26\n",
      "done current dataset\n",
      "reading in images for dataset: BB18\n",
      "all_image_filenames length: 75\n",
      "done current dataset\n",
      "reading in images for dataset: BB19\n",
      "all_image_filenames length: 72\n",
      "done current dataset\n",
      "reading in images for dataset: BB20\n",
      "all_image_filenames length: 96\n",
      "done current dataset\n",
      "reading in images for dataset: BB21\n",
      "all_image_filenames length: 78\n",
      "done current dataset\n",
      "reading in images for dataset: BB22\n",
      "all_image_filenames length: 123\n",
      "done current dataset\n",
      "reading in images for dataset: BB23\n",
      "all_image_filenames length: 192\n",
      "done current dataset\n",
      "reading in images for dataset: BB24\n",
      "all_image_filenames length: 62\n",
      "done current dataset\n",
      "reading in images for dataset: BB25\n",
      "all_image_filenames length: 10\n",
      "done current dataset\n",
      "reading in images for dataset: BB26\n",
      "all_image_filenames length: 142\n",
      "done current dataset\n",
      "reading in images for dataset: BB27\n",
      "all_image_filenames length: 46\n",
      "done current dataset\n",
      "reading in images for dataset: BB28\n",
      "all_image_filenames length: 73\n",
      "done current dataset\n",
      "reading in images for dataset: BB29\n",
      "all_image_filenames length: 20\n",
      "done current dataset\n",
      "reading in images for dataset: BB30\n",
      "all_image_filenames length: 450\n",
      "done current dataset\n"
     ]
    }
   ],
   "source": [
    "# get the all original output filenames\n",
    "def readInImages(datasetName):\n",
    "    print(\"reading in images for dataset: {}\".format(datasetName))\n",
    "    desired_size = 224\n",
    "    image_list = []\n",
    "    imgRegExp = re.compile(r'.*[.](JPG)$')\n",
    "    # https://stackoverflow.com/a/3207973\n",
    "    all_image_filenames = next(os.walk('data/{}'.format(datasetName)),\n",
    "                         (None, None, []))[2]  # [] if no file\n",
    "    # filter out file names that are not JPEGs\n",
    "    all_image_filenames = [i for i in all_image_filenames if imgRegExp.match(i)]\n",
    "    # walk() outputs unordered, so we need to sort\n",
    "    all_image_filenames.sort()\n",
    "    # print(\"all_image_filenames: {}\".format(all_image_filenames))\n",
    "    print(\"all_image_filenames length: {}\".format(len(all_image_filenames)))\n",
    "    for fn in all_image_filenames:\n",
    "        # im = Image.open('data/{}/{}'.format(datasetName, fn))\n",
    "        im = cv2.imread('data/{}/{}'.format(datasetName, fn))\n",
    "        # resize the image to conserve memory, and transform it to be square while\n",
    "        # maintaining the aspect ration (give it padding):\n",
    "        # https://jdhao.github.io/2017/11/06/resize-image-to-square-with-padding/#using-opencv\n",
    "        # im = cv2.resize(im, (480, 270), interpolation=cv2.INTER_CUBIC)\n",
    "        # im = cv2.resize(im, (240, 135), interpolation=cv2.INTER_CUBIC)\n",
    "        # im = cv2.resize(im, (192, 108), interpolation=cv2.INTER_CUBIC)\n",
    "        old_size = im.shape[:2]\n",
    "        ratio = float(desired_size)/max(old_size)\n",
    "        new_size = tuple([int(x*ratio) for x in old_size])\n",
    "        im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "\n",
    "        delta_w = desired_size - new_size[1]\n",
    "        delta_h = desired_size - new_size[0]\n",
    "        top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "        left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "\n",
    "        color = [0, 0, 0]\n",
    "        new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "\n",
    "        \n",
    "        image_list.append(np.asarray(new_im))\n",
    "    \n",
    "    print(\"done current dataset\")\n",
    "    return image_list\n",
    "\n",
    "all_images = []\n",
    "for fn in dataset_names:    \n",
    "    all_images = [*all_images, *readInImages(fn)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded9195-def1-48e6-bb54-4062ec01dc5b",
   "metadata": {},
   "source": [
    "# Read in dataset's labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14a993b-efcc-4927-b522-44cbb90e8295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all classes (length=8): {'Other', 'Human Presense/Deployment', 'Empty photo', 'Cat', 'Rabbit', 'Kangaroo', 'Emu', 'Fox'}\n"
     ]
    }
   ],
   "source": [
    "# labels (using dataset's CSV file)\n",
    "\n",
    "def readInAnnotations(datasetName):\n",
    "    labelList = []\n",
    "    # https://realpython.com/python-csv/#reading-csv-files-with-csv\n",
    "    with open('data/{}/{}.csv'.format(datasetName, datasetName)) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            # print(\"row: {}\".format(row))\n",
    "            # first row always contains this string, so ignore it\n",
    "            if \"RECONYX - MapView Professional\" in row:\n",
    "                continue\n",
    "            if line_count == 0:\n",
    "                # print(f'Column names are {\", \".join(row)}')\n",
    "                line_count += 1\n",
    "            else:\n",
    "                # print(\"Image Name: {}. Hit List: {}\".format(row[0], row[22].replace(\"\\n\", \", \")))\n",
    "                # FIXME handle when hitlist contains more than one item (e.g., BB06 IMG_512 has 'kangaroo' and 'empty photo') - sort of handled, need to make more dynamic\n",
    "                hit_list = row[22]\n",
    "                if hit_list == '':\n",
    "                    labelList.append(\"Empty photo\")\n",
    "                elif hit_list == 'Empty photo\\nHuman Presense/Deployment':\n",
    "                    labelList.append(\"Human Presense/Deployment\")\n",
    "                elif hit_list == 'Kangaroo\\nEmpty photo':\n",
    "                    labelList.append(\"Kangaroo\")\n",
    "                else:\n",
    "                    # FIXME: rendundant case?\n",
    "                    labelList.append(hit_list.replace(\"\\n\", \", \"))\n",
    "                line_count += 1\n",
    "    # print(\"returning labelList (length: {}): {}\".format(len(labelList), labelList))\n",
    "    # print(\"returning labelList of length: {}\".format(len(labelList)))\n",
    "    return labelList\n",
    "\n",
    "all_image_labels = []\n",
    "for fn in dataset_names:\n",
    "    all_image_labels = [*all_image_labels, *readInAnnotations(fn)]\n",
    "\n",
    "# print(\"all_image_labels: {}\".format(all_image_labels))\n",
    "\n",
    "classes = set(all_image_labels)\n",
    "print(\"all classes (length={}): {}\".format(len(classes), classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f21c8c-b002-41c6-8ee5-8c249bc7f2aa",
   "metadata": {},
   "source": [
    "# Randomly split the dataset and corresponding labels into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44e9f9bb-0adf-4b24-8bb7-6607dccd3ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_images size: 6263\n",
      "training_labels length: 5010\n",
      "test_labels length: 1253\n",
      "training_classes (length=8): {'Other', 'Empty photo', 'Human Presense/Deployment', 'Cat', 'Rabbit', 'Kangaroo', 'Emu', 'Fox'}\n",
      "test_classes (length=8): {'Other', 'Human Presense/Deployment', 'Empty photo', 'Cat', 'Rabbit', 'Kangaroo', 'Emu', 'Fox'}\n",
      "done stacking\n",
      "training_images shape: (5010, 224, 224, 3)\n",
      "test_images shape: (1253, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"all_images size: {}\".format(len(all_images)))\n",
    "# print(\"all_image_labels size: {}\".format(len(all_image_labels)))\n",
    "\n",
    "\n",
    "training_images, test_images, training_labels, test_labels = train_test_split(all_images, all_image_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"training_labels length: {}\".format(len(training_labels)))\n",
    "print(\"test_labels length: {}\".format(len(test_labels)))\n",
    "\n",
    "training_classes = set(training_labels)\n",
    "test_classes = set(test_labels)\n",
    "print(\"training_classes (length={}): {}\".format(len(training_classes), training_classes))\n",
    "print(\"test_classes (length={}): {}\".format(len(test_classes), test_classes))\n",
    "\n",
    "# integer-encode labels so they can be one-hot-encoded\n",
    "# https://stackoverflow.com/a/56227965/6476994\n",
    "label_encoder = LabelEncoder()\n",
    "training_labels = np.array(training_labels)\n",
    "training_labels = label_encoder.fit_transform(training_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "test_labels = label_encoder.fit_transform(test_labels)\n",
    "\n",
    "\n",
    "# convert list of numpy arrays to numpy array of numpy arrays\n",
    "# https://stackoverflow.com/a/27516930/6476994\n",
    "training_images = np.stack(training_images, axis = 0)\n",
    "test_images = np.stack(test_images, axis = 0)\n",
    "\n",
    "print(\"done stacking\")\n",
    "print(\"training_images shape: {}\".format(training_images.shape))\n",
    "print(\"test_images shape: {}\".format(test_images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de40603-e6cd-4c93-856c-eed237c3f7dc",
   "metadata": {},
   "source": [
    "# ZFNet\n",
    "\n",
    "Source: https://towardsdatascience.com/zfnet-an-explanation-of-paper-with-code-f1bd6752121d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da469ee2-ad90-402b-9b57-55a1388138e3",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d882d77-9109-494a-8c37-7ee3518adf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-28 09:53:16.690855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:53:16.693886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:53:16.694061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:53:16.694362: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-28 09:53:16.694746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:53:16.694894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:53:16.695026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:53:17.002052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:53:17.002227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:53:17.002366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-28 09:53:17.002487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5671 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:2b:00.0, compute capability: 7.5\n",
      "2022-05-28 09:53:17.476539: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3016581120 exceeds 10% of free system memory.\n",
      "/home/luke/miniconda3/envs/cv-project-env/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "2022-05-28 09:53:19.966554: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2413264896 exceeds 10% of free system memory.\n",
      "2022-05-28 09:53:20.738914: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2413264896 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-28 09:53:22.257741: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2022-05-28 09:53:22.584076: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 10s 214ms/step - loss: 1.8112 - accuracy: 0.5289 - top_k_categorical_accuracy: 0.9177 - val_loss: 1.2350 - val_accuracy: 0.6038 - val_top_k_categorical_accuracy: 0.9741 - lr: 0.0100\n",
      "Epoch 2/90\n",
      " 1/32 [..............................] - ETA: 5s - loss: 1.2151 - accuracy: 0.5859 - top_k_categorical_accuracy: 0.9922"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-28 09:53:31.342358: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 966.19MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2022-05-28 09:53:31.508043: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 966.19MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/32 [=>............................] - ETA: 4s - loss: 1.1288 - accuracy: 0.6328 - top_k_categorical_accuracy: 0.9792"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-28 09:53:31.675762: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 966.19MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2022-05-28 09:53:31.839310: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 966.19MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/32 [===>..........................] - ETA: 4s - loss: 1.1214 - accuracy: 0.6094 - top_k_categorical_accuracy: 0.9812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-28 09:53:32.005154: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 966.19MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2022-05-28 09:53:32.167605: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 966.19MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7/32 [=====>........................] - ETA: 4s - loss: 1.0991 - accuracy: 0.6228 - top_k_categorical_accuracy: 0.9810"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-28 09:53:32.333856: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 966.19MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2022-05-28 09:53:32.497502: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 966.19MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/32 [=======>......................] - ETA: 3s - loss: 1.0965 - accuracy: 0.6198 - top_k_categorical_accuracy: 0.9818"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-28 09:53:32.664828: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 966.19MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2022-05-28 09:53:32.828415: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 966.19MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 6s 175ms/step - loss: 1.0576 - accuracy: 0.6218 - top_k_categorical_accuracy: 0.9853 - val_loss: 1.0637 - val_accuracy: 0.5918 - val_top_k_categorical_accuracy: 0.9920 - lr: 0.0100\n",
      "Epoch 3/90\n",
      "32/32 [==============================] - 6s 174ms/step - loss: 0.9682 - accuracy: 0.6340 - top_k_categorical_accuracy: 0.9920 - val_loss: 1.0661 - val_accuracy: 0.6098 - val_top_k_categorical_accuracy: 0.9910 - lr: 0.0100\n",
      "Epoch 4/90\n",
      "32/32 [==============================] - 6s 175ms/step - loss: 1.0503 - accuracy: 0.6315 - top_k_categorical_accuracy: 0.9860 - val_loss: 0.9674 - val_accuracy: 0.6537 - val_top_k_categorical_accuracy: 0.9910 - lr: 1.0000e-03\n",
      "Epoch 5/90\n",
      "32/32 [==============================] - 6s 175ms/step - loss: 0.8708 - accuracy: 0.6722 - top_k_categorical_accuracy: 0.9925 - val_loss: 0.9024 - val_accuracy: 0.6697 - val_top_k_categorical_accuracy: 0.9910 - lr: 1.0000e-03\n",
      "Epoch 6/90\n",
      "32/32 [==============================] - 6s 175ms/step - loss: 0.8323 - accuracy: 0.6831 - top_k_categorical_accuracy: 0.9925 - val_loss: 0.8780 - val_accuracy: 0.6916 - val_top_k_categorical_accuracy: 0.9910 - lr: 1.0000e-03\n",
      "Epoch 7/90\n",
      "32/32 [==============================] - 6s 175ms/step - loss: 0.8149 - accuracy: 0.6951 - top_k_categorical_accuracy: 0.9925 - val_loss: 0.8472 - val_accuracy: 0.6866 - val_top_k_categorical_accuracy: 0.9910 - lr: 1.0000e-03\n",
      "Epoch 8/90\n",
      "32/32 [==============================] - 6s 174ms/step - loss: 0.7909 - accuracy: 0.7033 - top_k_categorical_accuracy: 0.9935 - val_loss: 0.8321 - val_accuracy: 0.6866 - val_top_k_categorical_accuracy: 0.9910 - lr: 1.0000e-03\n",
      "Epoch 9/90\n",
      "32/32 [==============================] - 6s 175ms/step - loss: 0.7714 - accuracy: 0.7111 - top_k_categorical_accuracy: 0.9938 - val_loss: 0.8176 - val_accuracy: 0.6996 - val_top_k_categorical_accuracy: 0.9930 - lr: 1.0000e-03\n",
      "Epoch 10/90\n",
      "32/32 [==============================] - 6s 175ms/step - loss: 0.7524 - accuracy: 0.7263 - top_k_categorical_accuracy: 0.9938 - val_loss: 0.8124 - val_accuracy: 0.7255 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-03\n",
      "Epoch 11/90\n",
      "32/32 [==============================] - 6s 175ms/step - loss: 0.7347 - accuracy: 0.7343 - top_k_categorical_accuracy: 0.9935 - val_loss: 0.7938 - val_accuracy: 0.7146 - val_top_k_categorical_accuracy: 0.9930 - lr: 1.0000e-03\n",
      "Epoch 12/90\n",
      "32/32 [==============================] - 6s 175ms/step - loss: 0.7190 - accuracy: 0.7418 - top_k_categorical_accuracy: 0.9943 - val_loss: 0.7788 - val_accuracy: 0.7385 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-03\n",
      "Epoch 13/90\n",
      "32/32 [==============================] - 6s 175ms/step - loss: 0.7055 - accuracy: 0.7470 - top_k_categorical_accuracy: 0.9940 - val_loss: 0.7662 - val_accuracy: 0.7525 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-03\n",
      "Epoch 14/90\n",
      "32/32 [==============================] - 6s 175ms/step - loss: 0.6942 - accuracy: 0.7570 - top_k_categorical_accuracy: 0.9943 - val_loss: 0.7422 - val_accuracy: 0.7535 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-03\n",
      "Epoch 15/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6809 - accuracy: 0.7580 - top_k_categorical_accuracy: 0.9948 - val_loss: 0.7328 - val_accuracy: 0.7515 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-03\n",
      "Epoch 16/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6803 - accuracy: 0.7527 - top_k_categorical_accuracy: 0.9945 - val_loss: 0.7395 - val_accuracy: 0.7595 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-03\n",
      "Epoch 17/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6741 - accuracy: 0.7522 - top_k_categorical_accuracy: 0.9945 - val_loss: 0.7267 - val_accuracy: 0.7545 - val_top_k_categorical_accuracy: 0.9920 - lr: 1.0000e-04\n",
      "Epoch 18/90\n",
      "32/32 [==============================] - 6s 175ms/step - loss: 0.6405 - accuracy: 0.7782 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.7079 - val_accuracy: 0.7585 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-04\n",
      "Epoch 19/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6308 - accuracy: 0.7802 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6988 - val_accuracy: 0.7675 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-04\n",
      "Epoch 20/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6256 - accuracy: 0.7814 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6974 - val_accuracy: 0.7565 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-04\n",
      "Epoch 21/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6231 - accuracy: 0.7837 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6931 - val_accuracy: 0.7595 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-04\n",
      "Epoch 22/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6221 - accuracy: 0.7817 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6944 - val_accuracy: 0.7525 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-04\n",
      "Epoch 23/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6198 - accuracy: 0.7792 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6932 - val_accuracy: 0.7605 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 24/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6167 - accuracy: 0.7854 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6911 - val_accuracy: 0.7615 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 25/90\n",
      "32/32 [==============================] - 6s 177ms/step - loss: 0.6158 - accuracy: 0.7857 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6904 - val_accuracy: 0.7625 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 26/90\n",
      "32/32 [==============================] - 6s 177ms/step - loss: 0.6156 - accuracy: 0.7839 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6902 - val_accuracy: 0.7605 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 27/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6152 - accuracy: 0.7854 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6897 - val_accuracy: 0.7625 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 28/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6150 - accuracy: 0.7854 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6897 - val_accuracy: 0.7615 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 29/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6147 - accuracy: 0.7867 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6896 - val_accuracy: 0.7615 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 30/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6145 - accuracy: 0.7857 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6893 - val_accuracy: 0.7615 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 31/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6144 - accuracy: 0.7862 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6890 - val_accuracy: 0.7605 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 32/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6143 - accuracy: 0.7844 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6886 - val_accuracy: 0.7625 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 33/90\n",
      "32/32 [==============================] - 6s 177ms/step - loss: 0.6139 - accuracy: 0.7859 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6885 - val_accuracy: 0.7615 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 34/90\n",
      "32/32 [==============================] - 6s 177ms/step - loss: 0.6137 - accuracy: 0.7849 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6886 - val_accuracy: 0.7615 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 35/90\n",
      "32/32 [==============================] - 6s 178ms/step - loss: 0.6134 - accuracy: 0.7849 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6882 - val_accuracy: 0.7615 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 36/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6134 - accuracy: 0.7862 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6879 - val_accuracy: 0.7615 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 37/90\n",
      "32/32 [==============================] - 6s 178ms/step - loss: 0.6132 - accuracy: 0.7869 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6881 - val_accuracy: 0.7635 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 38/90\n",
      "32/32 [==============================] - 6s 181ms/step - loss: 0.6130 - accuracy: 0.7859 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6878 - val_accuracy: 0.7635 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 39/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6127 - accuracy: 0.7842 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6875 - val_accuracy: 0.7625 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 40/90\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 0.6126 - accuracy: 0.7859 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6874 - val_accuracy: 0.7615 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 41/90\n",
      "32/32 [==============================] - 5s 172ms/step - loss: 0.6122 - accuracy: 0.7864 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6874 - val_accuracy: 0.7635 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 42/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6122 - accuracy: 0.7859 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6872 - val_accuracy: 0.7615 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 43/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6120 - accuracy: 0.7859 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6874 - val_accuracy: 0.7625 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 44/90\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 0.6116 - accuracy: 0.7867 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6867 - val_accuracy: 0.7615 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 45/90\n",
      "32/32 [==============================] - 5s 172ms/step - loss: 0.6115 - accuracy: 0.7867 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6869 - val_accuracy: 0.7625 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 46/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6114 - accuracy: 0.7864 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6869 - val_accuracy: 0.7625 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 47/90\n",
      "32/32 [==============================] - 5s 172ms/step - loss: 0.6112 - accuracy: 0.7874 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6864 - val_accuracy: 0.7625 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 48/90\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 0.6108 - accuracy: 0.7874 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6864 - val_accuracy: 0.7635 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 49/90\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 0.6107 - accuracy: 0.7869 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6861 - val_accuracy: 0.7625 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 50/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6106 - accuracy: 0.7867 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6860 - val_accuracy: 0.7635 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 51/90\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 0.6108 - accuracy: 0.7887 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6856 - val_accuracy: 0.7635 - val_top_k_categorical_accuracy: 0.9950 - lr: 1.0000e-05\n",
      "Epoch 52/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6101 - accuracy: 0.7877 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6854 - val_accuracy: 0.7635 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 53/90\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 0.6100 - accuracy: 0.7864 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6857 - val_accuracy: 0.7635 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 54/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6098 - accuracy: 0.7872 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6853 - val_accuracy: 0.7625 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 55/90\n",
      "32/32 [==============================] - 5s 172ms/step - loss: 0.6097 - accuracy: 0.7869 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6850 - val_accuracy: 0.7655 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 56/90\n",
      "32/32 [==============================] - 5s 172ms/step - loss: 0.6094 - accuracy: 0.7877 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6853 - val_accuracy: 0.7625 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 57/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6092 - accuracy: 0.7879 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6848 - val_accuracy: 0.7635 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 58/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6091 - accuracy: 0.7892 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6848 - val_accuracy: 0.7645 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 59/90\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 0.6087 - accuracy: 0.7892 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6848 - val_accuracy: 0.7645 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 60/90\n",
      "32/32 [==============================] - 5s 172ms/step - loss: 0.6089 - accuracy: 0.7874 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6845 - val_accuracy: 0.7645 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 61/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6084 - accuracy: 0.7882 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6840 - val_accuracy: 0.7645 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 62/90\n",
      "32/32 [==============================] - 5s 172ms/step - loss: 0.6083 - accuracy: 0.7889 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6840 - val_accuracy: 0.7665 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 63/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6080 - accuracy: 0.7887 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6842 - val_accuracy: 0.7645 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 64/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6080 - accuracy: 0.7879 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6842 - val_accuracy: 0.7635 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 65/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6076 - accuracy: 0.7887 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6836 - val_accuracy: 0.7645 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 66/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6076 - accuracy: 0.7884 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6834 - val_accuracy: 0.7665 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 67/90\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 0.6073 - accuracy: 0.7889 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6837 - val_accuracy: 0.7645 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 68/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6072 - accuracy: 0.7884 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6832 - val_accuracy: 0.7675 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 69/90\n",
      "32/32 [==============================] - 5s 172ms/step - loss: 0.6069 - accuracy: 0.7894 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6831 - val_accuracy: 0.7645 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 70/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6066 - accuracy: 0.7882 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6833 - val_accuracy: 0.7645 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 71/90\n",
      "32/32 [==============================] - 5s 172ms/step - loss: 0.6064 - accuracy: 0.7882 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6831 - val_accuracy: 0.7665 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 72/90\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 0.6062 - accuracy: 0.7889 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6828 - val_accuracy: 0.7645 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 73/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6062 - accuracy: 0.7892 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6825 - val_accuracy: 0.7685 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 74/90\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 0.6059 - accuracy: 0.7892 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6824 - val_accuracy: 0.7655 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 75/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6057 - accuracy: 0.7894 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6824 - val_accuracy: 0.7675 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 76/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6055 - accuracy: 0.7904 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6825 - val_accuracy: 0.7655 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 77/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6052 - accuracy: 0.7907 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6823 - val_accuracy: 0.7645 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 78/90\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 0.6051 - accuracy: 0.7909 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6820 - val_accuracy: 0.7675 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 79/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6049 - accuracy: 0.7909 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6820 - val_accuracy: 0.7675 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 80/90\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 0.6047 - accuracy: 0.7899 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6819 - val_accuracy: 0.7655 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 81/90\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 0.6046 - accuracy: 0.7899 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6813 - val_accuracy: 0.7675 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 82/90\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 0.6046 - accuracy: 0.7894 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6817 - val_accuracy: 0.7655 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 83/90\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 0.6043 - accuracy: 0.7904 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6811 - val_accuracy: 0.7655 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 84/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6040 - accuracy: 0.7902 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6813 - val_accuracy: 0.7655 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 85/90\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.6039 - accuracy: 0.7897 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6810 - val_accuracy: 0.7655 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 86/90\n",
      "32/32 [==============================] - 5s 172ms/step - loss: 0.6036 - accuracy: 0.7909 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6808 - val_accuracy: 0.7685 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 87/90\n",
      "32/32 [==============================] - 6s 175ms/step - loss: 0.6035 - accuracy: 0.7914 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6805 - val_accuracy: 0.7695 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 88/90\n",
      "32/32 [==============================] - 6s 174ms/step - loss: 0.6034 - accuracy: 0.7907 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6811 - val_accuracy: 0.7675 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 89/90\n",
      "32/32 [==============================] - 6s 175ms/step - loss: 0.6031 - accuracy: 0.7899 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6805 - val_accuracy: 0.7695 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n",
      "Epoch 90/90\n",
      "32/32 [==============================] - 6s 176ms/step - loss: 0.6030 - accuracy: 0.7899 - top_k_categorical_accuracy: 0.9953 - val_loss: 0.6803 - val_accuracy: 0.7665 - val_top_k_categorical_accuracy: 0.9940 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3e3c3e2a30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_images = tf.map_fn(lambda i: tf.stack([i]*3, axis=-1), training_images).numpy()\n",
    "# test_images = tf.map_fn(lambda i: tf.stack([i]*3, axis=-1), test_images).numpy()\n",
    "\n",
    "training_images = tf.image.resize(training_images, [224, 224]).numpy()\n",
    "test_images = tf.image.resize(test_images, [224, 224]).numpy()\n",
    "\n",
    "training_images = training_images.reshape(training_images.shape)\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images.reshape(test_images.shape)\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "training_labels = tf.keras.utils.to_categorical(training_labels, num_classes=len(training_classes))\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=len(test_classes))\n",
    "\n",
    "num_len_train = int(0.8 * len(training_images))\n",
    "\n",
    "ttraining_images = training_images[:num_len_train]\n",
    "ttraining_labels = training_labels[:num_len_train]\n",
    "\n",
    "valid_images = training_images[num_len_train:]\n",
    "valid_labels = training_labels[num_len_train:]\n",
    "\n",
    "training_images = ttraining_images\n",
    "training_labels = ttraining_labels\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "                                    \n",
    "\t\ttf.keras.layers.Conv2D(96, (7, 7), strides=(2, 2), activation='relu',\n",
    "\t\t\tinput_shape=(224, 224, 3)),\n",
    "\t\ttf.keras.layers.MaxPooling2D(3, strides=2),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.image.per_image_standardization(x)),\n",
    "\n",
    "\t\ttf.keras.layers.Conv2D(256, (5, 5), strides=(2, 2), activation='relu'),\n",
    "\t\ttf.keras.layers.MaxPooling2D(3, strides=2),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.image.per_image_standardization(x)),\n",
    "\n",
    "\t\ttf.keras.layers.Conv2D(384, (3, 3), activation='relu'),\n",
    "\n",
    "\t\ttf.keras.layers.Conv2D(384, (3, 3), activation='relu'),\n",
    "\n",
    "\t\ttf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "\n",
    "\t\ttf.keras.layers.MaxPooling2D(3, strides=2),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "\t\ttf.keras.layers.Dense(4096),\n",
    "\n",
    "\t\ttf.keras.layers.Dense(4096),\n",
    "\n",
    "\t\ttf.keras.layers.Dense(len(classes), activation='softmax')#FIXME is this the number of classes? (check paper)\n",
    "\t])\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.01, momentum=0.9), \\\n",
    "              loss='categorical_crossentropy', \\\n",
    "              metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(5)])\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \\\n",
    "                                            \t\tfactor=0.1, patience=1, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmin_lr=0.00001)\n",
    "\n",
    "try: \n",
    "    model.fit(training_images, training_labels, batch_size=128, \\\n",
    "          validation_data=(valid_images, valid_labels), \\\n",
    "\t\t\t\t\tepochs=90, callbacks=[reduce_lr])\n",
    "except:\n",
    "    print(\"Error training model\")\n",
    "finally:\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d43d1c-41ee-45f0-a341-3d2757eb5a12",
   "metadata": {},
   "source": [
    "## Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c21960-4a3d-46cb-8dfd-d3b8a8314ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_images shape: (1253, 224, 224, 3)\n",
      "test_labels shape: (1253, 8)\n",
      "40/40 [==============================] - 1s 19ms/step - loss: 0.6630 - accuracy: 0.7590 - top_k_categorical_accuracy: 0.9952\n"
     ]
    }
   ],
   "source": [
    "print('test_images shape: {}'.format(test_images.shape))\n",
    "print('test_labels shape: {}'.format(test_labels.shape))\n",
    "\n",
    "results = model.evaluate(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18b52e25-9d06-4e81-b6c7-d317e158b35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 15ms/step\n",
      "Predictions (shape: (1253, 8)):\n",
      "[[0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "predictions = (model.predict(test_images) > 0.5).astype(\"int32\")\n",
    "print(\"Predictions (shape: {}):\\n{}\".format(predictions.shape, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37a2dd7f-3232-4b86-84e0-4e4b7137660b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         7\n",
      "           1       0.80      0.36      0.50       206\n",
      "           2       0.82      0.55      0.66       136\n",
      "           3       1.00      0.04      0.07        28\n",
      "           4       0.56      0.38      0.45       105\n",
      "           5       0.84      0.91      0.88       756\n",
      "           6       0.00      0.00      0.00        11\n",
      "           7       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.82      0.70      0.76      1253\n",
      "   macro avg       0.50      0.28      0.32      1253\n",
      "weighted avg       0.80      0.70      0.72      1253\n",
      " samples avg       0.70      0.70      0.70      1253\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/miniconda3/envs/cv-project-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/luke/miniconda3/envs/cv-project-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, predictions))\n",
    "# TODO resolve classes to their names (current integers in left column)\n",
    "\n",
    "# from sklearn.metrics import precision_score\n",
    "# print(\"Precision score: {}\".format(precision_score(test_labels,predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738305b2-92f1-41c1-a4f2-07171823ec10",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "* use the current date/time so we can keep incrementation progress of the model as we re-run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14e4681e-b9b8-419c-9cd5-bfd3b0a1763f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model as: 'ZFNet-28-05-2022_10:01:46.h5'.'\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "dt_string = now.strftime('%d-%m-%Y_%H:%M:%S')\n",
    "print(\"saving model as: 'ZFNet-{}.h5'.'\".format(dt_string))\n",
    "\n",
    "model.save('ZFNet-{}.h5'.format(dt_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c0524-22ef-4ad4-8403-faec5dc9f5f4",
   "metadata": {},
   "source": [
    "## Free up the GPU's memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b4c59ac-35fd-420c-ab71-2dc2c7c457ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25b0b019-ff12-4c7f-9278-3c16774e1262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# Counter(training_labels.tolist())\n",
    "# Counter(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426f494-49a1-4bac-a74f-35cb460b0175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
