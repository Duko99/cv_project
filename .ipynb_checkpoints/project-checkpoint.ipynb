{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad62cb3-b86d-4a0a-8492-ef09608a4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix last dense layer (should be 10 but only accespts 8). Is it the number of classes? If so then nothing to fix\n",
    "# TODO: discuss how reading in dataset could be made multi-threaded (each dataset is read in on own thread then all joined together at the end)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import imageio as iio\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from numba import cuda  # https://stackoverflow.com/a/52354865/6476994\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4a177c-422b-48b4-b51f-ac05542a4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows all images to be displayed at once (else only displays the last call to plt.imshow())\n",
    "# https://stackoverflow.com/a/41210974\n",
    "def displayImage(image, caption = None, colour = None) -> None:\n",
    "    plt.figure()\n",
    "    if(colour != None):\n",
    "        plt.imshow(image, cmap=colour)\n",
    "    else:\n",
    "        plt.imshow(image)\n",
    "        \n",
    "    if(caption != None):\n",
    "        # display caption below picture (https://stackoverflow.com/a/51486361)\n",
    "        plt.figtext(0.5, 0.01, caption, wrap=True, horizontalalignment='center', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7119a4c7-53e6-47b3-a57c-c8c54758002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"BB01\", \"BB02\", \"BB03\", \"BB04\", \"BB05\", \"BB06\", \"BB07\", \"BB08\", \"BB09\", \"BB10\"]\n",
    "# dataset_names = [\"BB08\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2162b806-d6bd-47ba-acc3-3f7209a27997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up GPU if it didn't after the last run\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b197fb12-a89b-4f03-a28c-ca0cb72b9e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = tf.keras.datasets.mnist\n",
    "# (training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760dbfe7-ea61-44ed-bbbd-1d3d3b576c3f",
   "metadata": {},
   "source": [
    "# Read in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb46df-237c-4f68-b1af-5ee75f6b69b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in images for dataset: BB01\n",
      "all_image_filenames length: 285\n",
      "done current dataset\n",
      "reading in images for dataset: BB02\n",
      "all_image_filenames length: 45\n",
      "done current dataset\n",
      "reading in images for dataset: BB03\n",
      "all_image_filenames length: 230\n",
      "done current dataset\n",
      "reading in images for dataset: BB04\n",
      "all_image_filenames length: 999\n",
      "done current dataset\n",
      "reading in images for dataset: BB05\n",
      "all_image_filenames length: 189\n",
      "done current dataset\n",
      "reading in images for dataset: BB06\n",
      "all_image_filenames length: 1137\n",
      "done current dataset\n",
      "reading in images for dataset: BB07\n",
      "all_image_filenames length: 324\n"
     ]
    }
   ],
   "source": [
    "# get the all original output filenames\n",
    "def readInImages(datasetName):\n",
    "    print(\"reading in images for dataset: {}\".format(datasetName))\n",
    "    desired_size = 224\n",
    "    image_list = []\n",
    "    imgRegExp = re.compile(r'.*[.](JPG)$')\n",
    "    # https://stackoverflow.com/a/3207973\n",
    "    all_image_filenames = next(os.walk('data/{}'.format(datasetName)),\n",
    "                         (None, None, []))[2]  # [] if no file\n",
    "    # filter out file names that are not JPEGs\n",
    "    all_image_filenames = [i for i in all_image_filenames if imgRegExp.match(i)]\n",
    "    # walk() outputs unordered, so we need to sort\n",
    "    all_image_filenames.sort()\n",
    "    # print(\"all_image_filenames: {}\".format(all_image_filenames))\n",
    "    print(\"all_image_filenames length: {}\".format(len(all_image_filenames)))\n",
    "    for fn in all_image_filenames:\n",
    "        # im = Image.open('data/{}/{}'.format(datasetName, fn))\n",
    "        im = cv2.imread('data/{}/{}'.format(datasetName, fn))\n",
    "        # resize the image to conserve memory, and transform it to be square while\n",
    "        # maintaining the aspect ration (give it padding):\n",
    "        # https://jdhao.github.io/2017/11/06/resize-image-to-square-with-padding/#using-opencv\n",
    "        # im = cv2.resize(im, (480, 270), interpolation=cv2.INTER_CUBIC)\n",
    "        # im = cv2.resize(im, (240, 135), interpolation=cv2.INTER_CUBIC)\n",
    "        # im = cv2.resize(im, (192, 108), interpolation=cv2.INTER_CUBIC)\n",
    "        old_size = im.shape[:2]\n",
    "        ratio = float(desired_size)/max(old_size)\n",
    "        new_size = tuple([int(x*ratio) for x in old_size])\n",
    "        im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "\n",
    "        delta_w = desired_size - new_size[1]\n",
    "        delta_h = desired_size - new_size[0]\n",
    "        top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "        left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "\n",
    "        color = [0, 0, 0]\n",
    "        new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "\n",
    "        \n",
    "        image_list.append(np.asarray(new_im))\n",
    "    \n",
    "    print(\"done current dataset\")\n",
    "    return image_list\n",
    "\n",
    "all_images = []\n",
    "for fn in dataset_names:    \n",
    "    all_images = [*all_images, *readInImages(fn)]\n",
    "    \n",
    "# convert list of numpy arrays to numpy array of numpy arrays\n",
    "# https://stackoverflow.com/a/27516930/6476994\n",
    "# all_images = np.stack(all_images, axis = 0)\n",
    "\n",
    "# print(\"done stacking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eaa6fc-726e-4d9e-ab3c-142d78a34fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displayImage(all_images[0])\n",
    "# print(\"all_images_len: {}\".format(len(all_images)))\n",
    "\n",
    "# training_images = all_images[0:1708]\n",
    "# print(\"training_images length: {}\".format(len(training_images)))\n",
    "# print(\"training_images shape: {}\".format(training_images.shape))\n",
    "# test_images = all_images[1709:2137]\n",
    "# print(\"test_images length: {}\".format(len(test_images)))\n",
    "# print(\"test_images shape: {}\".format(test_images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded9195-def1-48e6-bb54-4062ec01dc5b",
   "metadata": {},
   "source": [
    "# Read in dataset's labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14a993b-efcc-4927-b522-44cbb90e8295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels (using dataset's CSV file)\n",
    "\n",
    "def readInAnnotations(datasetName):\n",
    "    labelList = []\n",
    "    # https://realpython.com/python-csv/#reading-csv-files-with-csv\n",
    "    with open('data/{}/{}.csv'.format(datasetName, datasetName)) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            # print(\"row: {}\".format(row))\n",
    "            # first row always contains this string, so ignore it\n",
    "            if \"RECONYX - MapView Professional\" in row:\n",
    "                continue\n",
    "            if line_count == 0:\n",
    "                # print(f'Column names are {\", \".join(row)}')\n",
    "                line_count += 1\n",
    "            else:\n",
    "                # print(\"Image Name: {}. Hit List: {}\".format(row[0], row[22].replace(\"\\n\", \", \")))\n",
    "                # FIXME handle when hitlist contains more than one item (e.g., BB06 IMG_512 has 'kangaroo' and 'empty photo') - sort of handled, need to make more dynamic\n",
    "                hit_list = row[22]\n",
    "                if hit_list == '':\n",
    "                    labelList.append(\"Empty photo\")\n",
    "                elif hit_list == 'Empty photo\\nHuman Presense/Deployment':\n",
    "                    labelList.append(\"Human Presense/Deployment\")\n",
    "                elif hit_list == 'Kangaroo\\nEmpty photo':\n",
    "                    labelList.append(\"Kangaroo\")\n",
    "                else:\n",
    "                    # FIXME: rendundant case?\n",
    "                    labelList.append(hit_list.replace(\"\\n\", \", \"))\n",
    "                line_count += 1\n",
    "    # print(\"returning labelList (length: {}): {}\".format(len(labelList), labelList))\n",
    "    # print(\"returning labelList of length: {}\".format(len(labelList)))\n",
    "    return labelList\n",
    "\n",
    "all_image_labels = []\n",
    "for fn in dataset_names:\n",
    "    all_image_labels = [*all_image_labels, *readInAnnotations(fn)]\n",
    "\n",
    "# print(\"all_image_labels: {}\".format(all_image_labels))\n",
    "\n",
    "classes = set(all_image_labels)\n",
    "print(\"all classes (length={}): {}\".format(len(classes), classes))\n",
    "              \n",
    "# training_labels = all_image_labels[0:1708]\n",
    "# training_classes = set(training_labels)\n",
    "# print(\"training_classes (length={}): {}\".format(len(training_classes), training_classes))\n",
    "# test_labels = all_image_labels[1709:2137]\n",
    "# test_classes = set(test_labels)\n",
    "# print(\"test_classes (length={}): {}\".format(len(test_classes), test_classes))\n",
    "\n",
    "# # integer-encode labels so they can be one-hot-encoded\n",
    "# # https://stackoverflow.com/a/56227965/6476994\n",
    "# label_encoder = LabelEncoder()\n",
    "# training_labels = np.array(training_labels)\n",
    "# training_labels = label_encoder.fit_transform(training_labels)\n",
    "# test_labels = np.array(test_labels)\n",
    "# test_labels = label_encoder.fit_transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f21c8c-b002-41c6-8ee5-8c249bc7f2aa",
   "metadata": {},
   "source": [
    "# Randomly split the dataset and corresponding labels into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9f9bb-0adf-4b24-8bb7-6607dccd3ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all_images size: {}\".format(len(all_images)))\n",
    "# print(\"all_image_labels size: {}\".format(len(all_image_labels)))\n",
    "\n",
    "\n",
    "training_images, test_images, training_labels, test_labels = train_test_split(all_images, all_image_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"training_labels length: {}\".format(len(training_labels)))\n",
    "print(\"test_labels length: {}\".format(len(test_labels)))\n",
    "\n",
    "training_classes = set(training_labels)\n",
    "test_classes = set(test_labels)\n",
    "print(\"training_classes (length={}): {}\".format(len(training_classes), training_classes))\n",
    "print(\"test_classes (length={}): {}\".format(len(test_classes), test_classes))\n",
    "\n",
    "# integer-encode labels so they can be one-hot-encoded\n",
    "# https://stackoverflow.com/a/56227965/6476994\n",
    "label_encoder = LabelEncoder()\n",
    "training_labels = np.array(training_labels)\n",
    "training_labels = label_encoder.fit_transform(training_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "test_labels = label_encoder.fit_transform(test_labels)\n",
    "\n",
    "\n",
    "# convert list of numpy arrays to numpy array of numpy arrays\n",
    "# https://stackoverflow.com/a/27516930/6476994\n",
    "training_images = np.stack(training_images, axis = 0)\n",
    "test_images = np.stack(test_images, axis = 0)\n",
    "\n",
    "print(\"done stacking\")\n",
    "print(\"training_images shape: {}\".format(training_images.shape))\n",
    "print(\"test_images shape: {}\".format(test_images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de40603-e6cd-4c93-856c-eed237c3f7dc",
   "metadata": {},
   "source": [
    "# ZFNet\n",
    "\n",
    "Source: https://towardsdatascience.com/zfnet-an-explanation-of-paper-with-code-f1bd6752121d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da469ee2-ad90-402b-9b57-55a1388138e3",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d882d77-9109-494a-8c37-7ee3518adf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_images = tf.map_fn(lambda i: tf.stack([i]*3, axis=-1), training_images).numpy()\n",
    "# test_images = tf.map_fn(lambda i: tf.stack([i]*3, axis=-1), test_images).numpy()\n",
    "\n",
    "training_images = tf.image.resize(training_images, [224, 224]).numpy()\n",
    "test_images = tf.image.resize(test_images, [224, 224]).numpy()\n",
    "\n",
    "training_images = training_images.reshape(3002, 224, 224, 3)\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images.reshape(751, 224, 224, 3)\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "training_labels = tf.keras.utils.to_categorical(training_labels, num_classes=len(training_classes))\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=len(test_classes))\n",
    "\n",
    "num_len_train = int(0.8 * len(training_images))\n",
    "\n",
    "ttraining_images = training_images[:num_len_train]\n",
    "ttraining_labels = training_labels[:num_len_train]\n",
    "\n",
    "valid_images = training_images[num_len_train:]\n",
    "valid_labels = training_labels[num_len_train:]\n",
    "\n",
    "training_images = ttraining_images\n",
    "training_labels = ttraining_labels\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "                                    \n",
    "\t\ttf.keras.layers.Conv2D(96, (7, 7), strides=(2, 2), activation='relu',\n",
    "\t\t\tinput_shape=(224, 224, 3)),\n",
    "\t\ttf.keras.layers.MaxPooling2D(3, strides=2),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.image.per_image_standardization(x)),\n",
    "\n",
    "\t\ttf.keras.layers.Conv2D(256, (5, 5), strides=(2, 2), activation='relu'),\n",
    "\t\ttf.keras.layers.MaxPooling2D(3, strides=2),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.image.per_image_standardization(x)),\n",
    "\n",
    "\t\ttf.keras.layers.Conv2D(384, (3, 3), activation='relu'),\n",
    "\n",
    "\t\ttf.keras.layers.Conv2D(384, (3, 3), activation='relu'),\n",
    "\n",
    "\t\ttf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "\n",
    "\t\ttf.keras.layers.MaxPooling2D(3, strides=2),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "\t\ttf.keras.layers.Dense(4096),\n",
    "\n",
    "\t\ttf.keras.layers.Dense(4096),\n",
    "\n",
    "\t\ttf.keras.layers.Dense(len(training_classes), activation='softmax')#FIXME is this the number of classes? (check paper)\n",
    "\t])\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.01, momentum=0.9), \\\n",
    "              loss='categorical_crossentropy', \\\n",
    "              metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(5)])\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \\\n",
    "                                            \t\tfactor=0.1, patience=1, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmin_lr=0.00001)\n",
    "\n",
    "model.fit(training_images, training_labels, batch_size=128, \\\n",
    "          validation_data=(valid_images, valid_labels), \\\n",
    "\t\t\t\t\tepochs=90, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d43d1c-41ee-45f0-a341-3d2757eb5a12",
   "metadata": {},
   "source": [
    "## Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c21960-4a3d-46cb-8dfd-d3b8a8314ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_images shape: {}'.format(test_images.shape))\n",
    "print('test_labels shape: {}'.format(test_labels.shape))\n",
    "\n",
    "results = model.evaluate(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b52e25-9d06-4e81-b6c7-d317e158b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (model.predict(test_images) > 0.5).astype(\"int32\")\n",
    "print(\"Predictions (shape: {}):\\n{}\".format(predictions.shape, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2dd7f-3232-4b86-84e0-4e4b7137660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, predictions))\n",
    "# TODO resolve classes to their names (current integers in left column)\n",
    "\n",
    "# from sklearn.metrics import precision_score\n",
    "# print(\"Precision score: {}\".format(precision_score(test_labels,predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4681e-b9b8-419c-9cd5-bfd3b0a1763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ZFNet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c0524-22ef-4ad4-8403-faec5dc9f5f4",
   "metadata": {},
   "source": [
    "## Free up the GPU's memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4c59ac-35fd-420c-ab71-2dc2c7c457ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0b019-ff12-4c7f-9278-3c16774e1262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# Counter(training_labels.tolist())\n",
    "# Counter(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426f494-49a1-4bac-a74f-35cb460b0175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
